import streamlit as st
from audio_recorder_streamlit import audio_recorder
from openai import OpenAI
import os
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# Streamlit UI
st.title("ðŸ˜±DALL-E 3 ç”»åƒç”Ÿæˆ")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
prompt = st.text_input("ç”»åƒã®èª¬æ˜Žã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š'a white siamese cat'ï¼‰")

if st.button("ç”»åƒç”Ÿæˆ"):
    if prompt:
        # APIã‚’å‘¼ã³å‡ºã—ã¦ç”»åƒã‚’ç”Ÿæˆ
        response = client.images.generate(
            model="dall-e-3",
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1
        )

        # ç”Ÿæˆã•ã‚ŒãŸç”»åƒã®URLã‚’å–å¾—
        image_url = response.data[0].url

        # ç”»åƒã‚’è¡¨ç¤º
        st.image(image_url)
    else:
        st.warning("ç”»åƒã®èª¬æ˜Žã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

#########################################
picture = st.camera_input("Take a picture")

if picture:
    st.image(picture)

#########################################
def write_audio_file(file_path, audio_bytes):
    with open(file_path, "wb") as audio_file:
        audio_file.write(audio_bytes)

audio_bytes = audio_recorder(
    text="click and speak>>>",
    recording_color="#e8b62c",
    neutral_color="#6aa36f",
    icon_name="microphone-lines",
    icon_size="3x",
    pause_threshold=2.0,
    sample_rate=41_000
)  

if audio_bytes:
    st.audio(audio_bytes, format="audio/wav")
    write_audio_file("recorded_audio.wav", audio_bytes)

    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=open("recorded_audio.wav", "rb"),
    )
    st.text(transcript.text)
